# HAFS Configuration
# Halext Agentic File System

[general]
refresh_interval = 5      # Seconds between background refreshes
show_hidden_files = false
default_editor = "nvim"

[theme]
# Halext Purple Gradient
primary = "#4C3B52"
secondary = "#9B59B6"
accent = "#E74C3C"
gradient_start = "#4C3B52"
gradient_end = "#000000"

[parsers]
# Log parser configurations

[parsers.gemini]
enabled = true
base_path = "~/.gemini/tmp"
max_items = 50

# Projects to track for AFS management (legacy list)
tracked_projects = []

# Structured project catalog for background agents
[[projects]]
name = "halext-org"
path = "~/Code/halext-org"
kind = "backend"
tags = ["halext", "api"]
tooling_profile = "read_only"

[[projects]]
name = "yaze"
path = "~/Code/yaze"
kind = "emulator"
tags = ["snes", "tooling"]
tooling_profile = "read_only"

[[projects]]
name = "barista"
path = "~/Code/barista"
kind = "service"
tags = ["infra"]
tooling_profile = "read_only"

[[projects]]
name = "cortex"
path = "~/Code/cortex"
kind = "service"
tags = ["infra"]
tooling_profile = "read_only"

[[projects]]
name = "oracle-code"
path = "~/Code/oracle-code"
kind = "snes"
tags = ["zelda", "romhack"]
tooling_profile = "read_only"

[[projects]]
name = "halext-code"
path = "~/Code/halext-code"
kind = "snes"
tags = ["zelda", "romhack"]
tooling_profile = "read_only"

[[tool_profiles]]
name = "read_only"
allow = ["rg", "rg_files", "rg_todos", "git_status", "git_branch", "git_log", "git_diff", "ls"]

[[tool_profiles]]
name = "build_only"
allow = [
  "rg", "rg_files", "rg_todos", "git_status", "git_branch", "git_log", "git_diff", "ls",
  "pytest", "npm_test", "pnpm_test", "cargo_test", "go_test", "make_test", "just_test",
  "npm_build", "pnpm_build", "cargo_build", "go_build", "make_build", "just_build",
]

[[tool_profiles]]
name = "infra_ops"
allow = [
  "rg", "rg_files", "rg_todos", "git_status", "git_branch", "git_log", "git_diff", "ls",
  "uname", "whoami", "uptime", "df", "du", "ps", "lsof", "tail", "journalctl", "log_show",
  "launchctl", "systemctl", "docker", "docker_compose", "kubectl", "ssh", "scp", "rsync",
  "curl", "ping",
]

[[execution_modes]]
name = "read_only"
tool_profile = "read_only"

[[execution_modes]]
name = "build_only"
tool_profile = "build_only"

[[execution_modes]]
name = "infra_ops"
tool_profile = "infra_ops"

default_execution_mode = "read_only"

[plugins]
# Add paths to your custom plugin packages here
# plugin_dirs = ["~/Code/hafs_google_internal/src"]

# Native C++ acceleration configuration
# All features are optional - falls back to Python/NumPy when disabled or unavailable
[native]
enabled = true                  # Master switch for all native acceleration

# Individual feature toggles (only apply when enabled=true and feature is compiled)
similarity = true               # SIMD-accelerated cosine similarity (ARM NEON)
hnsw_index = true               # HNSW approximate nearest neighbor index
quantization = true             # Int8/Float16 embedding quantization
simdjson = true                 # SIMD-accelerated JSON parsing (requires simdjson)
streaming_index = true          # Thread-safe real-time embedding index

# Embedding model preferences (via Ollama)
embedding_model = "embeddinggemma"      # Preferred model (768-dim, multilingual)
embedding_fallback = "nomic-embed-text" # Fallback if preferred unavailable 

# AFS directory configurations
[[afs_directories]]
name = "memory"
policy = "read_only"
description = "Long-term docs and specs"

[[afs_directories]]
name = "knowledge"
policy = "read_only"
description = "Reference materials"

[[afs_directories]]
name = "tools"
policy = "executable"
description = "Executable scripts"

[[afs_directories]]
name = "scratchpad"
policy = "writable"
description = "AI reasoning space"

[[afs_directories]]
name = "history"
policy = "read_only"
description = "Archived scratchpads"

[[backends]]
name = "gemini"
enabled = true
command = ["gemini"]

# Training data generation configuration
[training]
enabled = true
output_dir = "~/.context/training"
checkpoint_interval = 100  # Save checkpoint every N samples

# Teacher model configuration
teacher_provider = "gemini"
teacher_tier = "coding"  # fast, coding, reasoning, creative

# Quality thresholds
min_quality_score = 0.7
diversity_threshold = 0.95  # Max similarity for dedup (higher = more duplicates removed)

# Target models for export (Unsloth-compatible)
[training.targets]
"qwen2.5-coder-7b" = { template = "qwen", context_length = 32768 }
"qwen2.5-coder-14b" = { template = "qwen", context_length = 32768 }
"deepseek-coder-v2-lite" = { template = "chatml", context_length = 16384 }
"codellama-7b" = { template = "llama3", context_length = 16384 }
"codellama-13b" = { template = "llama3", context_length = 16384 }
"mistral-7b" = { template = "mistral", context_length = 32768 }

# Domain-specific configurations
[[training.domains]]
name = "asm"
enabled = true
generator = "AsmDataGenerator"
source = "unified_kb"  # Uses UnifiedALTTPKnowledge
interval_hours = 24
limit = 100

[[training.domains]]
name = "cpp"
enabled = true
generator = "CppDataGenerator"
source = "~/Code/yaze"
interval_hours = 48
limit = 50

[[training.domains]]
name = "text"
enabled = false  # Enable when text sources are configured
generator = "TextDataGenerator"
source = "~/.context/knowledge/verified"
interval_hours = 72
limit = 50

# Remote training nodes
# Configure nodes for distributed generation or training
# [[training.nodes]]
# name = "windows-gpu"
# host = "192.168.1.100"  # Or Tailscale IP
# port = 8765
# gpu = "5060TI"
# memory_gb = 16
