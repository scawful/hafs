# Training Configuration for medical-mechanica (GPU Server)
# Use D drive for large datasets and model storage

[paths]
# D drive paths (Windows)
datasets = "D:/hafs_training/datasets"
checkpoints = "D:/hafs_training/checkpoints"
logs = "D:/hafs_training/logs"
models = "D:/hafs_training/models"
temp = "D:/hafs_training/temp"

# Source code and knowledge bases (C drive is fine)
knowledge_bases = "C:/Users/Administrator/.context/knowledge"
embeddings = "C:/Users/Administrator/.context/embeddings"

[campaign]
# Default campaign settings
target_samples = 34500
quality_threshold = 0.7  # Overridden by per-domain thresholds (use None for auto)
checkpoint_interval = 100  # Save every N samples
parallel_workers = 100  # Increased for maximum throughput (Gemini API: 1000 RPM)
max_concurrent_generations = 100  # Parallel generation streams

# Domain-specific thresholds (validated 2025-12-21)
[campaign.thresholds]
asm = 0.4
gigaleak = 0.45
oracle = 0.4
yaze = 0.5
cpp = 0.5
errors = 0.3
text = 0.6

[training]
# Model training settings (when training on medical-mechanica)
base_model = "Qwen/Qwen2.5-Coder-14B-Instruct"
use_lora = true
lora_r = 64
lora_alpha = 128
batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 2e-4
num_epochs = 3
warmup_steps = 100

# Hardware settings
device = "cuda"
fp16 = true
gradient_checkpointing = true

[monitoring]
# Health check and cleanup
auto_checkpoint_cleanup = true
max_checkpoint_age_days = 7
disk_alert_threshold_gb = 50
memory_alert_threshold_percent = 85

[api]
# API rate limits for generation
gemini_rpm = 1000  # Requests per minute
gemini_tpm = 4000000  # Tokens per minute
max_retries = 3
retry_delay_seconds = 5

[gpu]
# GPU acceleration settings (medical-mechanica)
enabled = true
use_local_models = true  # Use Ollama on GPU instead of Gemini
ollama_host = "100.104.53.21"  # Tailscale IP
ollama_port = 11435  # llama.cpp server port
models = ["qwen3-14b"]  # Available models on GPU

# Fallback to Gemini if GPU unavailable
fallback_to_gemini = true

[hybrid]
# Hybrid GPU + API mode (RECOMMENDED)
# Intelligently routes between GPU (free) and API (paid) based on GPU load
enabled = true
gpu_threshold_low = 70.0   # Below this %: prefer GPU
gpu_threshold_high = 90.0  # Above this %: prefer API
monitor_interval_sec = 5   # GPU status cache TTL

# Expected cost savings: 50-80% vs API-only
# GPU utilization thresholds:
#   <70%: Route to GPU (FREE)
#   70-90%: Mix GPU + API based on load
#   >90%: Route to API (prevent GPU overload)
