# Website Monitoring & Knowledge Creation Agents - TEMPLATE
#
# NOTE: This is a template file. For actual deployment, copy this to your
# plugin directory (e.g., ~/.config/hafs/plugins/my_hafs_plugin/config/) and
# customize it with your specific URLs, endpoints, and credentials.
#
# Example:
#   cp config/website_monitoring_agents.toml ~/.config/hafs/plugins/my_hafs_plugin/config/
#   # Edit ~/.config/hafs/plugins/my_hafs_plugin/config/website_monitoring_agents.toml
#   # Run with: --config ~/.config/hafs/plugins/my_hafs_plugin/config/website_monitoring_agents.toml

[agents.websitehealthmonitor]
enabled = true
provider = "local"
schedule = "*/30 * * * *"  # Every 30 minutes
description = "Monitor uptime and response times for your websites"

[agents.websitehealthmonitor.tasks]
websites = [
    { name = "example.com", url = "https://example.com", alert_threshold_ms = 5000 },
    { name = "blog.example.com", url = "https://blog.example.com", alert_threshold_ms = 3000 },
    { name = "api.example.com", url = "https://api.example.com/health", alert_threshold_ms = 2000 },
]
alert_on_downtime = true
alert_on_slow_response = true
output_dir = "~/.context/monitoring/health"
report_dir = "~/.context/logs/health_monitor"
keep_metrics_days = 30

[agents.contentindexer]
enabled = true
provider = "local"
schedule = "0 3 * * *"  # Daily at 3 AM
description = "Crawl and index website content for semantic search"

[agents.contentindexer.tasks]
websites = [
    { name = "example.com", url = "https://example.com", max_depth = 3 },
    { name = "blog.example.com", url = "https://blog.example.com", max_depth = 2, type = "blog" },
    { name = "api.example.com", url = "https://api.example.com/docs", max_depth = 1 },
]
respect_robots_txt = false  # NOTE: Not yet implemented - using rate limiting for safety
rate_limit_seconds = 1.0  # Conservative: 1 second between requests
max_pages_per_site = 500
user_agent = "hafs-indexer/1.0"

# Embedding configuration
embedding_model = "embeddinggemma"
ollama_url = "http://127.0.0.1:11434"
batch_size = 50
chunk_size = 512
chunk_overlap = 50

# Output paths
output_dir = "~/.context/knowledge/websites"
report_dir = "~/.context/logs/content_indexer"

[agents.changedetector]
enabled = true
provider = "local"
schedule = "0 4 * * *"  # Daily at 4 AM (after ContentIndexer)
description = "Detect content changes and new posts on websites"

[agents.changedetector.tasks]
websites = [
    { name = "example.com", url = "https://example.com" },
    { name = "blog.example.com", url = "https://blog.example.com", rss = "https://blog.example.com/feed.xml" },
    { name = "api.example.com", url = "https://api.example.com" },
]

# Git repository monitoring on halext-server
git_repos = [
    { name = "example-web", path = "/home/youruser/projects/example-web" },
    { name = "example-api", path = "/home/youruser/projects/example-api" },
]

# SSH configuration for halext-server
ssh_host = "user@your-host"
ssh_key = "~/.ssh/id_ed25519"

# AI summarization
use_ai_summarization = true
ai_model = "qwen2.5:14b"
ollama_url = "http://127.0.0.1:11434"

# Output paths
output_dir = "~/.context/monitoring/changes"
report_dir = "~/.context/logs/change_detector"

[agents.linkvalidator]
enabled = true
provider = "local"
schedule = "0 2 * * 0"  # Weekly on Sunday at 2 AM
description = "Validate internal and external links on websites"

[agents.linkvalidator.tasks]
websites = [
    { name = "example.com", url = "https://example.com" },
    { name = "blog.example.com", url = "https://blog.example.com" },
    { name = "api.example.com", url = "https://api.example.com" },
]

# Validation settings
check_internal_links = true
check_external_links = true
external_timeout_seconds = 10
max_retries = 2
ignore_patterns = ["*.pdf", "mailto:*", "#*"]

# Output paths
output_dir = "~/.context/monitoring/links"
report_dir = "~/.context/logs/link_validator"

[logging]
level = "INFO"
format = '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
file = "~/.context/logs/website_agents.log"
max_size_mb = 10
backup_count = 5
